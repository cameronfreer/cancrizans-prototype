name: Workflow Analytics

on:
  schedule:
    # Weekly on Sundays at 22:00 UTC
    - cron: '0 22 * * 0'
  workflow_dispatch:

permissions:
  contents: read
  actions: read

jobs:
  analyze-workflows:
    name: Analyze Workflow Performance
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests python-dateutil

      - name: Analyze workflow runs
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          python << 'EOF' > analytics_report.md
          import os
          import requests
          import json
          from datetime import datetime, timedelta
          from collections import defaultdict
          from dateutil import parser

          token = os.environ['GH_TOKEN']
          repo = os.environ['GITHUB_REPOSITORY']

          headers = {
              'Authorization': f'token {token}',
              'Accept': 'application/vnd.github+json'
          }

          print("# ðŸ“Š Workflow Analytics Report")
          print(f"\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\n")

          # Get workflow runs from last 30 days
          since_date = (datetime.now() - timedelta(days=30)).isoformat()

          url = f'https://api.github.com/repos/{repo}/actions/runs'
          params = {
              'per_page': 100,
              'created': f'>={since_date}'
          }

          try:
              response = requests.get(url, headers=headers, params=params)
              response.raise_for_status()
              data = response.json()

              runs = data.get('workflow_runs', [])

              print(f"## Overview (Last 30 Days)\n")
              print(f"- **Total Workflow Runs:** {len(runs)}")

              # Count by status
              status_counts = defaultdict(int)
              for run in runs:
                  status_counts[run['conclusion'] or run['status']] += 1

              print(f"- **Successful:** {status_counts.get('success', 0)}")
              print(f"- **Failed:** {status_counts.get('failure', 0)}")
              print(f"- **Cancelled:** {status_counts.get('cancelled', 0)}")

              success_rate = (status_counts.get('success', 0) / len(runs) * 100) if runs else 0
              print(f"- **Success Rate:** {success_rate:.1f}%\n")

              # Analyze by workflow
              print("## ðŸ“ˆ Performance by Workflow\n")

              workflow_stats = defaultdict(lambda: {
                  'total': 0,
                  'success': 0,
                  'failure': 0,
                  'duration': [],
                  'name': ''
              })

              for run in runs:
                  workflow_id = run['workflow_id']
                  workflow_name = run['name']

                  workflow_stats[workflow_id]['total'] += 1
                  workflow_stats[workflow_id]['name'] = workflow_name

                  if run['conclusion'] == 'success':
                      workflow_stats[workflow_id]['success'] += 1
                  elif run['conclusion'] == 'failure':
                      workflow_stats[workflow_id]['failure'] += 1

                  # Calculate duration
                  if run['run_started_at'] and run['updated_at']:
                      try:
                          start = parser.parse(run['run_started_at'])
                          end = parser.parse(run['updated_at'])
                          duration = (end - start).total_seconds()
                          if duration > 0:
                              workflow_stats[workflow_id]['duration'].append(duration)
                      except:
                          pass

              # Sort by total runs
              sorted_workflows = sorted(
                  workflow_stats.items(),
                  key=lambda x: x[1]['total'],
                  reverse=True
              )

              print("| Workflow | Runs | Success | Failure | Success Rate | Avg Duration |")
              print("|----------|------|---------|---------|--------------|--------------|")

              for wf_id, stats in sorted_workflows[:15]:  # Top 15
                  name = stats['name'][:30]
                  total = stats['total']
                  success = stats['success']
                  failure = stats['failure']

                  rate = (success / total * 100) if total > 0 else 0

                  if stats['duration']:
                      avg_duration = sum(stats['duration']) / len(stats['duration'])
                      duration_str = f"{int(avg_duration/60)}m {int(avg_duration%60)}s"
                  else:
                      duration_str = "N/A"

                  # Status emoji
                  if rate >= 95:
                      emoji = "âœ…"
                  elif rate >= 80:
                      emoji = "âš ï¸"
                  else:
                      emoji = "âŒ"

                  print(f"| {emoji} {name} | {total} | {success} | {failure} | {rate:.1f}% | {duration_str} |")

              # Find slowest workflows
              print("\n## ðŸŒ Slowest Workflows (Average Duration)\n")

              slow_workflows = []
              for wf_id, stats in workflow_stats.items():
                  if stats['duration']:
                      avg_duration = sum(stats['duration']) / len(stats['duration'])
                      slow_workflows.append((stats['name'], avg_duration))

              slow_workflows.sort(key=lambda x: x[1], reverse=True)

              print("| Workflow | Average Duration |")
              print("|----------|------------------|")
              for name, duration in slow_workflows[:10]:
                  duration_str = f"{int(duration/60)}m {int(duration%60)}s"
                  print(f"| {name[:40]} | {duration_str} |")

              # Find most active days
              print("\n## ðŸ“… Activity Pattern\n")

              daily_runs = defaultdict(int)
              for run in runs:
                  try:
                      date = parser.parse(run['created_at']).strftime('%Y-%m-%d')
                      daily_runs[date] += 1
                  except:
                      pass

              if daily_runs:
                  avg_daily = sum(daily_runs.values()) / len(daily_runs)
                  max_day = max(daily_runs.items(), key=lambda x: x[1])

                  print(f"- **Average runs per day:** {avg_daily:.1f}")
                  print(f"- **Peak day:** {max_day[0]} ({max_day[1]} runs)")

              # Cost estimation (rough)
              print("\n## ðŸ’° Resource Usage Estimate\n")

              total_minutes = 0
              for wf_id, stats in workflow_stats.items():
                  if stats['duration']:
                      total_minutes += sum(stats['duration']) / 60

              print(f"- **Total compute time:** {int(total_minutes)} minutes")
              print(f"- **Hours per day:** {total_minutes / 30 / 60:.1f} hours")

              # For public repos, GitHub Actions is free for public repos
              # For private repos, rough cost estimate
              print(f"- **Estimated cost (private repo):** ~${total_minutes * 0.008:.2f}/month")
              print("  *(Free for public repositories)*")

              # Recommendations
              print("\n## ðŸ’¡ Optimization Recommendations\n")

              recommendations = []

              # Check for high failure rates
              for wf_id, stats in workflow_stats.items():
                  if stats['total'] >= 5:  # Only check workflows with enough data
                      rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
                      if rate < 80:
                          recommendations.append(
                              f"- âš ï¸ **{stats['name']}** has low success rate ({rate:.1f}%) - investigate failures"
                          )

              # Check for slow workflows
              for name, duration in slow_workflows[:3]:
                  if duration > 600:  # > 10 minutes
                      recommendations.append(
                          f"- ðŸŒ **{name}** is slow ({int(duration/60)}m) - consider optimizing"
                      )

              if not recommendations:
                  print("âœ… **No major issues detected!** Workflows are performing well.")
              else:
                  for rec in recommendations[:5]:  # Top 5 recommendations
                      print(rec)

              print("\n---")
              print("\n*This report analyzes workflow performance to help optimize CI/CD efficiency.*")

          except Exception as e:
              print(f"Error analyzing workflows: {e}")
              import traceback
              print("\nTraceback:")
              print(traceback.format_exc())
          EOF

          cat analytics_report.md

      - name: Post to step summary
        run: |
          cat analytics_report.md >> $GITHUB_STEP_SUMMARY

      - name: Upload analytics report
        uses: actions/upload-artifact@v4
        with:
          name: workflow-analytics-${{ github.run_number }}
          path: analytics_report.md
          retention-days: 90

      - name: Analyze workflow files
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ“ Workflow Configuration Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python << 'EOF' >> $GITHUB_STEP_SUMMARY
          from pathlib import Path
          import re

          workflow_dir = Path('.github/workflows')

          if not workflow_dir.exists():
              print("âš ï¸ No workflows directory found")
          else:
              workflows = list(workflow_dir.glob('*.yml'))
              print(f"- **Total Workflow Files:** {len(workflows)}")

              # Analyze triggers
              triggers = set()
              schedule_count = 0

              for wf_file in workflows:
                  content = wf_file.read_text()

                  # Find triggers
                  if 'push:' in content:
                      triggers.add('push')
                  if 'pull_request:' in content:
                      triggers.add('pull_request')
                  if 'schedule:' in content:
                      triggers.add('schedule')
                      schedule_count += 1
                  if 'workflow_dispatch:' in content:
                      triggers.add('workflow_dispatch')
                  if 'release:' in content:
                      triggers.add('release')

              print(f"\n**Trigger Types in Use:**")
              for trigger in sorted(triggers):
                  count = sum(1 for wf in workflows if trigger + ':' in wf.read_text())
                  print(f"- `{trigger}`: {count} workflows")

              print(f"\n**Scheduled Workflows:** {schedule_count}")

              # Check for concurrency controls
              with_concurrency = sum(1 for wf in workflows if 'concurrency:' in wf.read_text())
              print(f"**Workflows with concurrency control:** {with_concurrency}/{len(workflows)}")

              # Check for caching
              with_cache = sum(1 for wf in workflows if 'actions/cache@' in wf.read_text())
              print(f"**Workflows using caching:** {with_cache}/{len(workflows)}")
          EOF

      - name: Generate recommendations
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸŽ¯ Actionable Insights" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python << 'EOF' >> $GITHUB_STEP_SUMMARY
          from pathlib import Path

          workflow_dir = Path('.github/workflows')
          workflows = list(workflow_dir.glob('*.yml'))

          insights = []

          # Check each workflow
          for wf_file in workflows:
              content = wf_file.read_text()

              # Check for missing concurrency control
              if ('push:' in content or 'pull_request:' in content) and 'concurrency:' not in content:
                  insights.append(f"- Consider adding concurrency control to `{wf_file.name}` to save resources")

              # Check for missing caching
              if 'pip install' in content and 'actions/cache@' not in content:
                  insights.append(f"- Add dependency caching to `{wf_file.name}` for faster runs")

              # Check for workflow_dispatch
              if 'workflow_dispatch:' not in content and wf_file.name not in ['cleanup.yml', 'stale.yml']:
                  insights.append(f"- Add `workflow_dispatch` to `{wf_file.name}` for manual triggering")

          if insights:
              print("**Configuration Improvements:**\n")
              for insight in insights[:8]:  # Top 8
                  print(insight)
          else:
              print("âœ… Workflow configurations are well optimized!")
          EOF
